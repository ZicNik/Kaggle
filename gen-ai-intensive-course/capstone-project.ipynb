{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11496295,"sourceType":"datasetVersion","datasetId":7088335}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/giocon/gen-ai-intensive-course-capstone-2025q1?scriptVersionId=236705877\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Automating content creation\n\nThis project uses Gemini APIs to generate a blog post about the project itself.\n\n\nThe idea had its birth shortly after I had read the [competition's rules](https://www.kaggle.com/competitions/gen-ai-intensive-course-capstone-2025q1). Something about them was disturbing to me: I realized that to achieve a decent score, the blog and YouTube video multipliers were a big boost. Even though I'm obviously not aiming for the top—to me this is more a way to experiment with Gen AI (also, I could not afford to put more effort than I have already put)—I'm still kind of a competitive fellow.\n\n\nThe disturbing part was that I'm lazy about doing these kind of things. I mean, I love them but only if they are done by someone other than me. You get the point: let the AI do the job!","metadata":{"_kg_hide-input":false}},{"cell_type":"markdown","source":"## Setup\n\nInstall the SDK and other tools for this notebook, then import the package and set up a retry policy so you don't have to manually retry when you hit a quota limit.","metadata":{}},{"cell_type":"code","source":"!pip uninstall -qyy jupyterlab\n!pip install -qU \"google-genai==1.9.0\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\n\nfrom IPython.display import Markdown, HTML\n\ngenai.__version__","metadata":{"trusted":true,"_kg_hide-input":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### API key\n\nTo run the following cell, your API key must be stored it in a [Kaggle secret](https://www.kaggle.com/discussions/product-feedback/114053) named `GOOGLE_API_KEY`.\n\nIf you don't already have an API key, you can grab one from [AI Studio](https://aistudio.google.com/app/apikey). You can find [detailed instructions in the docs](https://ai.google.dev/gemini-api/docs/api-key).\n\nTo make the key available through Kaggle secrets, choose `Secrets` from the `Add-ons` menu and follow the instructions to add your key or enable it for this notebook.","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\nclient = genai.Client(api_key=GOOGLE_API_KEY)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"If you received an error response along the lines of `No user secrets exist for kernel id ...`, then you need to add your API key via `Add-ons`, `Secrets` **and** enable it.\n\n![Screenshot of the checkbox to enable GOOGLE_API_KEY secret](https://storage.googleapis.com/kaggle-media/Images/5gdai_sc_3.png)","metadata":{}},{"cell_type":"markdown","source":"### Automated retry\n\nSet up a retry helper. This allows you to \"Run all\" without worrying about per-minute quota.","metadata":{}},{"cell_type":"code","source":"from google.api_core import retry\n\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\ngenai.models.Models.generate_content = retry.Retry(\n    predicate=is_retriable)(genai.models.Models.generate_content)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Retrieve the document","metadata":{}},{"cell_type":"code","source":"import os\n\n# Load the notebook\nnotebook_path = '/kaggle/input/gen-ai-intensive-course-capstone-2025q1-data/notebook.ipynb'\ntry:\n  with open(notebook_path, 'r') as f:\n      notebook = f.read()\n  print(\"Notebook content loaded successfully from file.\")\n\nexcept FileNotFoundError:\n    print(f\"Error: File not found at {notebook_path}\")\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This helper function strips the notebook from the parts that are not relevant for our purpose.","metadata":{}},{"cell_type":"code","source":"import json\n\ndef strip_notebook(json_string):\n    \"\"\"Returns the notebook's cells, with only the relevant content.\"\"\"\n    nb = json.loads(json_string)\n    cells = [{\n        \"cell_type\": cell[\"cell_type\"],\n        \"source\": cell[\"source\"]\n    } for cell in nb[\"cells\"]]\n    return json.dumps(cells)\n\nnotebook = strip_notebook(notebook)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Establish the evaluation rubric\n\nIn this part, we'll make so that our evaluations are grounded on the official source of information: the page of the competition itself. We will let the model extract the rubric for us.\n\n\nSince the page loads its content dynamically, we need to simulate the access from a web browser.","metadata":{}},{"cell_type":"code","source":"!pip install playwright\n!playwright install chromium","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from playwright.async_api import async_playwright\n\nasync def get_dynamic_content(url_string, wait_for=[]):\n    \"\"\"\n    Returns the dynamic content of a url, waiting for it to load.\n    \n    :param wait_for: Selectors identifying the elements required to be loaded\n    \"\"\"\n    async with async_playwright() as p:\n        browser = await p.chromium.launch()\n        page = await browser.new_page()\n        if wait_for:\n            await page.goto(url_string, wait_until='domcontentloaded')\n            for selector in wait_for:\n                await page.wait_for_selector(selector)\n        else:\n            await page.goto(url_string, wait_until='networkidle')\n        content = await page.content()\n        await browser.close()\n        return content\n\nhtml = await get_dynamic_content(\n    'https://www.kaggle.com/competitions/gen-ai-intensive-course-capstone-2025q1',\n    wait_for=['#evaluation'] # We previously inspected that the rubric is mostly within the element with id=\"evaluation\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt = ('Get the evaluation rubric from this html.\\n'\n          'INSTRUCTIONS\\n'\n          '* Be concise and schematic.\\n'\n          '* Add referenced information, if available and relevant.\\n'\n          '* Avoid introduction, comments, and final considerations.\\n'\n          '* Stay grounded to the source.\\n')\n\noverall_rubric = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        temperature=0\n    ),\n    contents=[prompt, html]).text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Markdown(overall_rubric)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now that we know the overall parameters to adhere to, let's dive into the specifics of this project: here we define a rubric for the evaluation of the final output, the generated blog.","metadata":{}},{"cell_type":"code","source":"blog_rubric = \"\"\"\\\nPoor:\n    - Structure & Clarity: Lacks structure; ideas are scattered or hard to follow. Sentences may be convoluted or poorly written.\n    - Technical Communication: Either extremely vague or overburdened with code-level details. Little to no explanation of core ideas.\n    - Audience Awareness: Fails to consider the reader; uses unexplained jargon or alternatively avoids all technical language entirely.\n    - Tone & Style: Dry or confusing. Attempts at humor (if any) feel forced or inappropriate. The text may be overly pedantic or tedious to read.\n\t- Added Value: Simply restates the code or project without adding perspective, insights, or broader context.\nFair:\n    - Structure & Clarity: Some logical flow, but still rough or awkward. Paragraphs may lack transitions or focus.\n    - Technical Communication: Describes some methods but may dwell too much on details or skip key concepts.\n    - Audience Awareness: Partial awareness of audience, but either too formal or too casual. Explains some technical terms, but inconsistently.\n\t- Tone & Style: Attempt at readability is evident but limited. Humor is absent or not well integrated. The post may still feel dry or overly verbose.\n\t- Added Value: Offers a minimal layer of interpretation or commentary beyond the notebook.\nSatisfactory:\n    - Structure & Clarity: Clear enough to follow. Structure is recognizable (intro, body, conclusion), though it may feel mechanical.\n    - Technical Communication: Balances explanation and code references reasonably well. Some depth, though not always well synthesized.\n    - Audience Awareness: Moderate understanding of audience; some jargon is explained, some not.\n\t- Tone & Style: Professional tone with a few moments of personality. Readable, though it may still lean toward textbook-like or too cautious.\n\t- Added Value: Provides context, some insights, and shows effort to guide the reader through key parts of the project.\nGood:\n    - Structure & Clarity: Well-organized and coherent. Flows logically and transitions are smooth.\n    - Technical Communication: Explains key methods and reasoning clearly without over-explaining. Good abstraction over technical details.\n    - Audience Awareness: Calibrated for an informed reader. Balances clarity and depth; jargon is introduced carefully.\n    - Tone & Style: Fluid and engaging. Some personality shines through. The post is easy to read and maintains reader interest.\n    - Added Value: Reflects on choices, presents challenges or alternatives, and conveys the broader meaning or potential of the project.\nExcellent:\n    - Structure & Clarity: Exceptionally well-structured. Each section builds on the previous, guiding the reader naturally through the narrative.\n    - Technical Communication: Strikes an excellent balance: technical enough to be informative, abstract enough to remain accessible.\n    - Audience Awareness: Crystal-clear understanding of who the reader is. Uses analogies or narrative devices where helpful without dumbing down.\n    - Tone & Style: Smooth, concise, yet expressive. Possibly includes humor or witty remarks that enhance readability and approachability.\n    - Added Value: Demonstrates deep insight and reflection. Brings the project to life, goes beyond surface-level commentary, and sparks curiosity.\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Since we don't need the reasonig behind the assignement of the rating—the rubric is already very detailed—we will force the evaluation to the following structured output.","metadata":{"_kg_hide-input":false}},{"cell_type":"code","source":"from enum import Enum\n\nclass BlogRating(Enum):\n    POOR = 'Poor'\n    FAIR = 'Fair'\n    SATISFACTORY = 'Satisfactory'\n    GOOD = 'Good'\n    EXCELLENT = 'Excellent'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Evaluate the document *(optional)*\n\nThis is not the main point of the project, but since the competition goals are pretty clear and well-structured, why not let the model help us achieving the best possible result?","metadata":{}},{"cell_type":"code","source":"instructions = 'You are an expert evaluator. Your task is to rate a document, according to a provided rubric.'\n\nprompt = \"\"\"\\\n# INPUTS\n1. The document.\n2. The rubric.\n\n# INSTRUCTIONS\n1. Start with a global evaluation.\n2. Highlight strengths.\n3. Highlight weaknesses.\n4. Briefly suggest possible improvements.\n5. Give a score to the notebook only.\n\"\"\"\n\nnotebook_evaluation = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        system_instruction=instructions,\n        temperature=0\n    ),\n    contents=[prompt, notebook, overall_rubric]).text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Markdown(notebook_evaluation)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Blog generation\n\nThis is the pivotal part of the project. Let's try to build the most effective prompt towards the desired result.","metadata":{}},{"cell_type":"code","source":"def generate_blog():\n    \"\"\"Generates a blog of this notebook, based on a set of instructions and evaluation criteria.\"\"\"\n    instructions = 'You are a navigated blogger, passionate about coding and AI.'\n    prompt =\"\"\"\\\n    Write me a blog post about this Kaggle notebook.\n    \n    INSTRUCTIONS\n    - Skip the \"Setup\" part of the notebook.\n    - Stick to the blog part of the \"overall rubric\".\n    - Maximize the rating according to the \"blog rubric\".\n    \n    OVERALL RUBRIC\n    {overall_rubric}\n    \n    BLOG RUBRIC\n    {blog_rubric}\n    \"\"\"\n    return client.models.generate_content(\n        model='gemini-2.0-flash',\n        config=types.GenerateContentConfig(\n            system_instruction=instructions,\n            temperature=0.2 # Raise the temperature a little bit, to enhance creativity\n        ),\n        contents=[prompt.format(overall_rubric=overall_rubric, blog_rubric=blog_rubric), notebook]).text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We now let the model generate the blog and then evaluate its own product. We make sure that we get only the best: if it's not up to our standards then... let's try again!\n\n\nAs already discussed, we're not interested in the reasoning behind the evaluation. It is just meant for the final rating, so we will enforce the structure of its output—thus also sparing precious tokens.","metadata":{}},{"cell_type":"code","source":"def evaluate_blog(blog, rubric):\n    \"\"\"Evaluates the blog according to a rubric, and returns a summary rating.\"\"\"\n    prompt = \"\"\"\\\n    You are an expert evaluator. Your task is to rate a technical blog. The result is a single rating, avaraging the rubric's criteria.\n    \n    # BLOG\n    {blog}\n    \n    # RUBRIC\n    {rubric}\n    \"\"\"\n    \n    return client.models.generate_content(\n        model='gemini-2.0-flash',\n        config=types.GenerateContentConfig(\n            temperature=0,\n            response_mime_type=\"text/x.enum\",\n            response_schema=BlogRating,\n        ),\n        contents=[prompt.format(blog=blog, rubric=rubric)]).parsed\n\nmax_tries = 10\ntryn = 0\nblog = generate_blog()\nblog_evaluation = evaluate_blog(blog, blog_rubric)\nblog_backup = None\nwhile blog_evaluation is not BlogRating.EXCELLENT:\n    print(\"Discarded blog rated: {}.\".format(blog_evaluation))\n    if blog_evaluation is BlogRating.GOOD:\n        blog_backup = blog\n    tryn += 1\n    if tryn == max_tries:\n        break\n    blog = generate_blog()\n    blog_evaluation = evaluate_blog(blog, blog_rubric)\n\n# If no 'Excellent' blog was generated, backup to a 'Good' blog if possible\nif tryn == max_tries:\n    if blog_backup is None:\n        raise Exception('Gave up generating a blog: quality is too poor...')\n    else:\n        print(\"Recovered last 'Good' blog.\".format(blog_evaluation))\n        blog = blog_backup","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Markdown(blog)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Save the blog to the output folder.","metadata":{}},{"cell_type":"code","source":"with open(\"blog.md\", \"w\", encoding=\"utf-8\") as f:\n    f.write(blog)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Final thoughts\n\nAnd this is the end of the project. To me, it has been a very enjoyable journey. But, before ending, I want to briefly discuss about the following.\n\n### Things that didn't work too well\n\nInitially I tried to use the Google search grounding feature to retrieve some required web content (the information in the competion page). But, honestly, the results that I got were not satisfactory. I had to opt for using the playwright library instead.\n\n### Other applications\n\nThe fun part of the project is this form of self-reflection, where the code that conjures the AI is then used and evaluated by the conjured AI itself. But of course, the fundamental steps (1. Retrieve the document, 2. Establish the evaluation rubric, 3. Blog generation) are easily abstracted and generalized to other applications. For instance, to write a blog/documentation of another given document.\n\n### Further directions\n\nWith the same logic, I would have loved to use Gen AI to produce also the YouTube video about the project. But, other than I could not afford it in terms of time, the task is evidently more complex than the blog creation. At the time of writing, as far as I know there is not a Gemini model well suited to video generation (at least a free of charge one). I was thinking to overcome this difficulty by doing something like a slide show: let the AI produce an audio file and a set of images, which are then to be assembled in a second moment. The not easy part, in my mind, is to make the correct prompts so that the generated files are coherent with each other. But I suppose that I will reserve this idea for the next time.\n\nThank you so much for your attention. Bye!","metadata":{"execution":{"iopub.status.busy":"2025-04-21T06:35:30.129791Z","iopub.execute_input":"2025-04-21T06:35:30.130123Z","iopub.status.idle":"2025-04-21T06:35:30.137664Z","shell.execute_reply.started":"2025-04-21T06:35:30.130098Z","shell.execute_reply":"2025-04-21T06:35:30.136168Z"}}}]}